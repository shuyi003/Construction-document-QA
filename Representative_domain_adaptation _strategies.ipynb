{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOtznJlC8g+D26XdLMNSzR3"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"22oJAvp0gWM9"},"outputs":[],"source":["# Strategy 1: Prompt engineering\n","\n","! pip install -U langchain-community\n","\n","import os\n","import json\n","import time\n","import numpy as np\n","import nltk\n","from nltk.tokenize import word_tokenize\n","from langchain.chat_models import ChatOpenAI\n","\n","nltk.download(\"punkt\")\n","\n","# Set OpenAI API Key\n","os.environ[\"OPENAI_API_KEY\"] = \"API_key\"\n","\n","def load_mixed_qa_dataset(filepath):\n","    \"\"\"\n","    Loads and processes a mixed QA dataset containing both context-based and open QA.\n","\n","    Returns:\n","        - A merged dataset where each sample includes 'context' (if applicable), 'question', and 'answer'.\n","    \"\"\"\n","    try:\n","        with open(filepath, \"r\", encoding=\"utf-8\") as f:\n","            data = json.load(f)\n","    except Exception as e:\n","        print(f\"Error loading file {filepath}: {e}\")\n","        return []\n","\n","    merged_qa_context = []\n","    for item in data:\n","        context = item.get(\"context\", \"\").strip()  # Extract context (if available)\n","        if \"questions\" in item:\n","            for qa_pair in item[\"questions\"]:\n","                question = qa_pair.get(\"question\", \"\").strip()\n","                answer_text = qa_pair.get(\"answer\", {}).get(\"text\", \"\").strip()  # Extract only answer text\n","\n","                if question and answer_text:\n","                    merged_qa_context.append({\n","                        \"context\": context,  # Store the context\n","                        \"question\": question,\n","                        \"answer\": answer_text\n","                    })\n","        else:\n","            question = item.get(\"Question\", \"\").strip()\n","            answer_text = item.get(\"Answer\", \"\").strip()\n","\n","            if question and answer_text:\n","                merged_qa_context.append({\n","                    \"context\": None,  # No context available\n","                    \"question\": question,\n","                    \"answer\": answer_text\n","                })\n","\n","    print(f\"Successfully processed {len(merged_qa_context)} samples from {filepath}\")\n","    return merged_qa_context\n","\n","mixed_qa_path = \"file_path\"\n","merged_qa_context = load_mixed_qa_dataset(mixed_qa_path)\n","\n","# Prompt engineering format\n","def evaluate_gpt_on_dataset(dataset, save_path=\"file_path\"):\n","    gpt4_model = ChatOpenAI(model=\"gpt-4o\", temperature=0.0)\n","\n","    for idx, sample in enumerate(dataset):\n","        question = sample[\"question\"].strip()\n","        input_context = sample.get(\"context\", \"\").strip() if sample.get(\"context\") else \"\"\n","        ground_truth = sample[\"answer\"].strip()\n","        if isinstance(ground_truth, dict):\n","            ground_truth = ground_truth.get(\"text\", \"\").strip()\n","\n","        if not ground_truth:\n","            continue\n","\n","        is_context_based = bool(input_context)\n","\n","        if is_context_based:\n","            query = (\n","                \"You are an expert in answering questions from construction specifications.\\n\"\n","                \"Below are some examples of QA pairs from construction specifications:\\n\"\n","                \"Example 1:\\n\"\n","                \"Context: The Contractor shall arrange surfacing operations so that the placing of materials will be accomplished during daylight hours. However, when necessary to complete the project within the time specified, or to avoid peak periods of public traffic, work may be undertaken during the hours of darkness, provided the Contractor furnishes and operates adequate lighting.\\n\"\n","                \"Question: Who is responsible for arranging surfacing operations?\\n\"\n","                \"Answer: The Contractor\\n\"\n","                \"Example 2:\\n\"\n","                \"Context: No specific unit of measurement shall apply to the lump sum item of shoring or extra excavation Class A. Shoring or extra excavation Class B will be measured by the square foot as follows: The area for payment will be one vertical plane measured along the centerline of the trench, including Structures. Measurement will be made from the existing ground line to the bottom of the excavation and for the length of the Work actually performed. If the Contract includes a pay item for grading to remove materials, the upper limit for measurement will be the neat lines of the grading section shown in the Plans. The bottom elevation for measurement will be the bottom of the excavation as shown in the Plans or as otherwise established by the Engineer.\\n\"\n","                \"Question: Who establishes the bottom elevation for measurement in the excavation plans?\\n\"\n","                \"Answer: the Engineer\\n\"\n","                f\"Now, answer the following question based on the given context and the answer should be word, phrase, and sentence from the input context. Just output the final answer directly.:\\n\"\n","                f\"Context: {input_context}\\n\"\n","                f\"Question: {question}\\n\"\n","                \"Answer:\"\n","            )\n","        else:\n","            query = (\n","                \"You are an expert at answering construction safety guideline questions.\\n\"\n","                \"Below are some examples of QA pairs for construction safety:\\n\"\n","                \"Example 1:\\n\"\n","                \"Question: What safety precautions should I take when transporting portable lighting fixtures?\\n\"\n","                \"Answer: Portable lighting fixtures must be equipped with protective netting, insulated, and grounded.\\n\"\n","                \"Example 2:\\n\"\n","                \"Question: What factors should be considered when planning equipment use?\\n\"\n","                \"Answer: Consider equipment characteristics, work content, usage methods, surrounding environment, and transport routes to ensure safe operations.\\n\"\n","                f\"Now, answer the following question concisely within 30 words. Just output the final answer directly.:\\n\"\n","                f\"Question: {question}\\n\"\n","                \"Just output the final answer directly. Answer:\"\n","            )\n","        generated_answer = gpt4_model.predict(query).strip()"]},{"cell_type":"code","source":["# Strategy 2: Hybrid RAG\n","\n","! pip install rank-bm25 chromadb transformers datasets torch evaluate sentence-transformers rouge-score nltk\n","! pip install -U langchain-community\n","\n","import os\n","import json\n","import time\n","import numpy as np\n","import nltk\n","from langchain.document_loaders import TextLoader\n","from langchain.text_splitter import RecursiveCharacterTextSplitter\n","from rank_bm25 import BM25Okapi\n","from nltk.tokenize import word_tokenize\n","from langchain.chat_models import ChatOpenAI\n","\n","nltk.download(\"punkt\")\n","\n","# Set OpenAI API Key\n","os.environ[\"OPENAI_API_KEY\"] = \"API_key\"\n","\n","# Chunk documents\n","import os\n","from langchain.document_loaders import DirectoryLoader, TextLoader\n","from langchain.text_splitter import RecursiveCharacterTextSplitter\n","\n","def load_and_chunk_documents(directory_path, chunk_size=512, overlap=50):\n","    \"\"\"\n","    Loads multiple text files from a directory, chunks them, and returns processed text segments.\n","\n","    Args:\n","        directory_path (str): Path to the directory containing text files.\n","        chunk_size (int): Maximum number of tokens per chunk.\n","        overlap (int): Number of overlapping tokens between chunks.\n","\n","    Returns:\n","        List[str]: List of sentence-based chunks.\n","    \"\"\"\n","    # Load all text files from the directory\n","    loader = DirectoryLoader(directory_path, glob=\"*.txt\", loader_cls=TextLoader)\n","    documents = loader.load()\n","\n","    # Split documents into chunks\n","    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=overlap)\n","    chunks = text_splitter.split_documents(documents)\n","\n","    print(f\"Loaded {len(documents)} documents, split into {len(chunks)} chunks\")\n","    return chunks\n","\n","directory_path = \"file_path\"\n","document_chunks = load_and_chunk_documents(directory_path)\n","\n","# Indexing system\n","def create_hybrid_index(chunks):\n","    \"\"\"\n","    Creates BM25 and FAISS indexes from document chunks.\n","    \"\"\"\n","    # Tokenize chunks for BM25\n","    tokenized_chunks = [word_tokenize(chunk.page_content.lower()) for chunk in chunks]\n","    bm25 = BM25Okapi(tokenized_chunks)\n","\n","    # Convert documents into dense embeddings for FAISS\n","    embedder = SentenceTransformer(\"all-mpnet-base-v2\")\n","    doc_embeddings = embedder.encode([chunk.page_content for chunk in chunks], convert_to_numpy=True)\n","\n","    # Build FAISS index\n","    dim = doc_embeddings.shape[1]\n","    faiss_index = faiss.IndexFlatL2(dim)\n","    faiss_index.add(doc_embeddings)\n","\n","    print(\"BM25 & FAISS indexes created successfully!\")\n","    return bm25, tokenized_chunks, chunks, faiss_index, embedder\n","bm25, tokenized_chunks, document_chunks, faiss_index, embedder = create_hybrid_index(document_chunks)\n","print(\"BM25 & FAISS indexes successfully created and tested\")\n","\n","# Retrieval system\n","def retrieve_relevant_docs_hybrid(query, bm25, tokenized_chunks, original_chunks, faiss_index, embedder, top_k=3, bm25_weight=0.4, faiss_weight=0.6):\n","    \"\"\"\n","    Retrieves relevant documents using a hybrid BM25 + FAISS approach.\n","    Scores from both methods are weighted to return the best matches.\n","    \"\"\"\n","    # BM25 Retrieval\n","    query_tokens = word_tokenize(query.lower())\n","    bm25_scores = np.array(bm25.get_scores(query_tokens))\n","\n","    # FAISS Dense Retrieval\n","    query_embedding = embedder.encode([query], convert_to_numpy=True)\n","    D, I = faiss_index.search(query_embedding, top_k)\n","\n","    # Convert FAISS scores to full-size array\n","    faiss_scores = np.zeros(len(original_chunks))\n","    for idx, score in zip(I[0], np.exp(-D[0])):\n","        faiss_scores[idx] = score\n","\n","    # Normalize Scores (Avoid Division by Zero)\n","    if np.max(bm25_scores) > 0:\n","        bm25_scores = (bm25_scores - np.min(bm25_scores)) / (np.max(bm25_scores) - np.min(bm25_scores) + 1e-8)\n","    if np.max(faiss_scores) > 0:\n","        faiss_scores = (faiss_scores - np.min(faiss_scores)) / (np.max(faiss_scores) - np.min(faiss_scores) + 1e-8)\n","\n","    # Compute Hybrid Score\n","    hybrid_scores = bm25_weight * bm25_scores + faiss_weight * faiss_scores\n","\n","    # Get Top-K Documents\n","    top_indices = np.argsort(hybrid_scores)[::-1][:top_k]\n","    retrieved_docs = [original_chunks[i] for i in top_indices]\n","\n","\n","    print(f\"Retrieved {len(retrieved_docs)} documents using Hybrid BM25+FAISS\")\n","    for i, doc in enumerate(retrieved_docs):\n","        print(f\"Document {i+1}:\\n{doc.page_content[:300]}...\\n\")\n","\n","    return retrieved_docs\n","\n","# Generation mechanism\n","def generate_answer_with_hybrid(question, input_context=\"\", use_context=False, bm25=None, tokenized_chunks=None, document_chunks=None, faiss_index=None, embedder=None):\n","    \"\"\"\n","    Retrieves relevant documents using BM25 + FAISS Hybrid retrieval and generates an answer using GPT-4o.\n","    \"\"\"\n","    retrieval_query = f\"{input_context} {question}\" if use_context else question\n","    retrieved_docs = retrieve_relevant_docs_hybrid(retrieval_query, bm25, tokenized_chunks, document_chunks, faiss_index, embedder, top_k=3)\n","    retrieved_context = \" \".join([doc.page_content for doc in retrieved_docs]) if retrieved_docs else \"No relevant context found\"\n","    if use_context:\n","        query = f\"Input Context: {input_context}\\n\"\n","        query += f\"Retrieved Context: {retrieved_context}\\n\" if retrieved_context else \"\"\n","        query += f\"Question: {question}\\nThe answer should be words, phrases, or sentences from the input context. Answer:\"\n","    else:\n","        query = f\"Retrieved Context: {retrieved_context}\\n\"\n","        query += f\"Question: {question}\\nAnswer:\"\n","\n","    print(\"\\n Final Query to GPT-4o:\")\n","    print(query)\n","    gpt4_model = ChatOpenAI(model=\"gpt-4o\", temperature=0.0)\n","    generated_answer = gpt4_model.predict(query).strip()\n","\n","    return generated_answer"],"metadata":{"id":"ehnPgvhxgj40"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Strategy 3: Task-specific fine-tuning\n","\n","%cd /content/\n","%rm -rf LLaMA-Factory\n","!git clone --depth 1 https://github.com/hiyouga/LLaMA-Factory.git\n","%cd LLaMA-Factory\n","%ls\n","!pip install -e .[torch,bitsandbytes]\n","\n","import json\n","\n","%cd /content/LLaMA-Factory/\n","\n","NAME = \"Llama-3\"\n","AUTHOR = \"LLaMA Factory\"\n","\n","with open(\"data/identity.json\", \"r\", encoding=\"utf-8\") as f:\n","  dataset = json.load(f)\n","\n","for sample in dataset:\n","  sample[\"output\"] = sample[\"output\"].replace(\"{{\"+ \"name\" + \"}}\", NAME).replace(\"{{\"+ \"author\" + \"}}\", AUTHOR)\n","\n","with open(\"data/identity.json\", \"w\", encoding=\"utf-8\") as f:\n","  json.dump(dataset, f, indent=2, ensure_ascii=False)\n","\n","\n","import json\n","\n","args = dict(\n","  stage=\"sft\",\n","  do_train=True,\n","  model_name_or_path=\"unsloth/llama-3-8b-Instruct-bnb-4bit\",\n","  dataset=\"identity, instruction_data_finetuning_llama\",\n","  template=\"llama3\",\n","  finetuning_type=\"lora\",\n","  lora_target=\"all\",\n","  output_dir=\"file_path\",\n","  per_device_train_batch_size=,\n","  gradient_accumulation_steps=,\n","  lr_scheduler_type=,\n","  logging_steps=,\n","  warmup_ratio=,\n","  save_steps=,\n","  learning_rate=,\n","  num_train_epochs=,\n","  max_samples=,\n","  max_grad_norm=,\n","  loraplus_lr_ratio=,\n","  fp16=True,\n","  report_to=\"none\",\n",")\n","\n","json.dump(args, open(\"train_dataset.json\", \"w\", encoding=\"utf-8\"), indent=2)"],"metadata":{"id":"ctc3zyRTiGVt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Strategy 4: Pretraining-and-fine-tuning\n","\n","! pip install transformers datasets accelerate\n","\n","# Pretraining on construction-specific corpus\n","file_paths = [\"fine_path\"]\n","corpus = []\n","for file_path in file_paths:\n","    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n","        corpus.append(file.read())\n","full_corpus = \"\\n\".join(corpus)\n","print(f\"Sample Text: {full_corpus[:500]}\")\n","\n","from datasets import Dataset\n","dataset = Dataset.from_dict({\"text\": corpus})\n","import os\n","import random\n","from datasets import Dataset\n","from transformers import T5Tokenizer\n","import numpy as np\n","from transformers import T5Tokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments, DataCollatorForSeq2Seq\n","\n","tokenizer = T5Tokenizer.from_pretrained(\"t5-small/medium/large\")\n","\n","corpus_dir = \"file_path\"\n","\n","def load_text_files(directory):\n","    file_paths = [os.path.join(directory, file) for file in os.listdir(directory) if file.endswith(\".txt\")]\n","    print(f\"Found {len(file_paths)} files for processing.\")\n","\n","    all_texts = []\n","    for file_path in file_paths:\n","        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n","            text = f.read().strip()\n","            if text:  # Ensure non-empty text\n","                all_texts.append({\"text\": text})\n","\n","    return all_texts\n","\n","text_data = load_text_files(corpus_dir)\n","dataset = Dataset.from_list(text_data)\n","\n","def mask_spans(text, mask_ratio=0.15):\n","    tokens = text.split()\n","    num_tokens = len(tokens)\n","    num_masked = max(1, int(mask_ratio * num_tokens))\n","\n","    if num_masked >= num_tokens:\n","        return \"<extra_id_0>\", \" \".join(tokens)\n","\n","    masked_indices = np.random.choice(num_tokens, num_masked, replace=False)\n","    masked_tokens = np.array(tokens, dtype=object)\n","    masked_tokens[masked_indices] = [f\"<extra_id_{i}>\" for i in range(num_masked)]\n","\n","    masked_input = \" \".join(masked_tokens)\n","    masked_output = \" \".join(tokens[i] for i in masked_indices)\n","\n","    return masked_input, masked_output\n","\n","def format_autoregressive(text):\n","    return text.strip(), text.strip()\n","\n","def preprocess_data_batch(batch):\n","    input_texts, target_texts = [], []\n","\n","    for text in batch[\"text\"]:\n","        if random.random() < 0.5:\n","            input_text, target_text = mask_spans(text)\n","        else:\n","            input_text, target_text = format_autoregressive(text)\n","\n","        input_texts.append(input_text)\n","        target_texts.append(target_text)\n","\n","    return {\"input_text\": input_texts, \"output_text\": target_texts}\n","\n","dataset = dataset.map(preprocess_data_batch, batched=True, num_proc=4)\n","\n","def tokenize_function(batch):\n","    inputs = tokenizer(batch[\"input_text\"], padding=\"max_length\", truncation=True, max_length=512)\n","    targets = tokenizer(batch[\"output_text\"], padding=\"max_length\", truncation=True, max_length=512)\n","\n","    return {\n","        \"input_ids\": inputs[\"input_ids\"],\n","        \"attention_mask\": inputs[\"attention_mask\"],\n","        \"labels\": targets[\"input_ids\"],\n","    }\n","\n","tokenized_dataset = dataset.map(tokenize_function, batched=True, num_proc=4)  # Parallel tokenization\n","\n","split_dataset = tokenized_dataset.train_test_split(test_size=0.1)\n","train_dataset, val_dataset = split_dataset[\"train\"], split_dataset[\"test\"]\n","\n","data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n","from transformers import T5ForConditionalGeneration, Trainer, TrainingArguments\n","\n","model = T5ForConditionalGeneration.from_pretrained(\"t5-small/medium/large\")\n","\n","training_args = TrainingArguments(\n","    output_dir=\"file_path\",\n","    per_device_train_batch_size=,\n","    num_train_epochs=,\n","    save_steps=,\n","    save_total_limit=,\n","    logging_dir=\"file_path\",\n","    logging_steps=,\n","    learning_rate=,\n","    weight_decay=,\n","    warmup_steps=,\n","    fp16=True\n",")\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=train_dataset,\n","    eval_dataset=val_dataset,\n","    tokenizer=tokenizer,\n","    data_collator=data_collator,\n",")\n","\n","trainer.train()\n","\n","# Fine-tuning on construction QA dataset\n","\n","import json\n","import torch\n","from datasets import Dataset, DatasetDict\n","from transformers import T5Tokenizer, T5ForConditionalGeneration, TrainingArguments, Trainer\n","\n","\n","with open('training_dataset.json', 'r', encoding='utf-8') as train_file:\n","    train_data = json.load(train_file)\n","\n","with open('testing_dataset.json', 'r', encoding='utf-8') as test_file:\n","    test_data = json.load(test_file)\n","\n","def convert_to_hf_dataset(data):\n","    questions = []\n","    answers = []\n","\n","    for entry in data:\n","        if \"context\" in entry:\n","            context = entry[\"context\"]\n","            for qa in entry[\"questions\"]:\n","                question = qa[\"question\"]\n","                answer = qa[\"answer\"][\"text\"]\n","                questions.append(f\"question: {question} context: {context}\")\n","                answers.append(answer)\n","        elif \"Question\" in entry and \"Answer\" in entry:\n","            question = entry[\"Question\"]\n","            answer = entry[\"Answer\"]\n","            questions.append(f\"question: {question}\")  # No context\n","            answers.append(answer)\n","\n","    return Dataset.from_dict({\"question\": questions, \"answer\": answers})\n","\n","train_dataset = convert_to_hf_dataset(train_data)\n","test_dataset = convert_to_hf_dataset(test_data)\n","\n","dataset = DatasetDict({\"train\": train_dataset, \"test\": test_dataset})\n","\n","tokenizer = T5Tokenizer.from_pretrained('construction_specialized_pretrained_T5')\n","\n","def preprocess_function(examples):\n","    inputs = examples[\"question\"]\n","    targets = examples[\"answer\"]\n","\n","    model_inputs = tokenizer(inputs, max_length=512, padding=\"max_length\", truncation=True)\n","    labels = tokenizer(targets, max_length=128, padding=\"max_length\", truncation=True)\n","\n","    model_inputs[\"labels\"] = labels[\"input_ids\"]\n","    return model_inputs\n","\n","tokenized_datasets = dataset.map(preprocess_function, batched=True, remove_columns=[\"question\", \"answer\"])\n","\n","model = T5ForConditionalGeneration.from_pretrained('construction_specialized_pretrained_T5')\n","\n","training_args = TrainingArguments(\n","    output_dir=\"output_path\",\n","    evaluation_strategy=\"epoch\",\n","    save_strategy=\"epoch\",\n","    logging_steps=,\n","    learning_rate=,\n","    per_device_train_batch_size=,\n","    per_device_eval_batch_size=,\n","    num_train_epochs=,\n","    weight_decay=,\n","    save_total_limit=,\n","    gradient_accumulation_steps=,\n","    eval_accumulation_steps=,\n","    load_best_model_at_end=True,\n","    metric_for_best_model=\"eval_loss\",\n","    greater_is_better=False\n",")\n","\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=tokenized_datasets[\"train\"],\n","    eval_dataset=tokenized_datasets[\"test\"],\n",")\n","\n","trainer.train()\n","\n","results = trainer.evaluate()\n"],"metadata":{"id":"f4KbMLN_iJ35"},"execution_count":null,"outputs":[]}]}